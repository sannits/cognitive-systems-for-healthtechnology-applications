{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4. Create directories and datasets\n",
    "Cognitive Systems for Health Technology Applications<br>\n",
    "Sakari Lukkarinen & Juha Kopu, 6.2.2018<br>\n",
    "[Helsinki Metropolia University of Applied Sciences](http://metropolia.fi/en)\n",
    "\n",
    "## Introduction\n",
    "This is a helper script that:\n",
    "1. Checks how many samples are in the original downloaded dataset\n",
    "2. Creates the directory structure for the datasets\n",
    "3. Creates the directories\n",
    "4. Splits the data into train, validation, and test sets\n",
    "5. Copies the data into the directories\n",
    "\n",
    "Before running the script you need to download the dataset from: https://github.com/Nomikxyz/retinopathy-dataset and extract the data to your computer. \n",
    "\n",
    "This script assumes that the original dataset is extracted to a folder named `retinopathy-dataset-master` and this script is in the sub-sub-folder under `Documents` \n",
    "\n",
    "The final directory structure when the script is executed is the following:\n",
    "```\n",
    "Documents/\n",
    "    retinopathy-dataset-master/\n",
    "        nosymptoms/\n",
    "        symptoms/\n",
    "    Python scripts/\n",
    "        Case2/\n",
    "            This script.ipynb\n",
    "    dataset2/\n",
    "        test/\n",
    "            nosymptoms/\n",
    "            symptoms/\n",
    "        evaluation/\n",
    "            nosymptoms/\n",
    "            symptoms/\n",
    "        validation/\n",
    "            nosymptoms/\n",
    "            symptoms/\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check the downloaded data\n",
    "First check that we have access to the downloaded and extracted data. This script reads the filenames in `nosymptoms` and `sympotms` subfoldes and counts how many observations are there totally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1468, 595)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all filenames in the master dataset and count how many samples there are\n",
    "original_dir = '\\\\Users\\\\Sanni Tolonen\\\\Documents\\\\Kolmosvuoden jutut\\\\Cognitive Applications\\\\retinopathy-dataset-master'\n",
    "\n",
    "class1 = 'nosymptoms'\n",
    "original_nosymptoms_dir = os.path.join(original_dir, class1)\n",
    "nosymptoms_fnames = os.listdir(original_nosymptoms_dir)\n",
    "\n",
    "class2 = 'symptoms'\n",
    "original_symptoms_dir = os.path.join(original_dir, class2)\n",
    "symptoms_fnames = os.listdir(original_symptoms_dir)\n",
    "\n",
    "len(nosymptoms_fnames), len(symptoms_fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Directory structure\n",
    "Next we create the names for the directories and save them into variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base directory is where the datasets will be created\n",
    "base_dir = '..\\\\..\\\\dataset2'\n",
    "\n",
    "# For training set\n",
    "sub_dir = 'train'\n",
    "train_dir = os.path.join(base_dir, sub_dir)\n",
    "train_nosymptoms_dir = os.path.join(base_dir, sub_dir, class1)\n",
    "train_symptoms_dir = os.path.join(base_dir, sub_dir, class2)\n",
    "\n",
    "# For validation set\n",
    "sub_dir = 'validation'\n",
    "validation_dir = os.path.join(base_dir, sub_dir)\n",
    "validation_nosymptoms_dir = os.path.join(base_dir, sub_dir, class1)\n",
    "validation_symptoms_dir = os.path.join(base_dir, sub_dir, class2)\n",
    "\n",
    "# For test set\n",
    "sub_dir = 'test'\n",
    "test_dir = os.path.join(base_dir, sub_dir)\n",
    "test_nosymptoms_dir = os.path.join(base_dir, sub_dir, class1)\n",
    "test_symptoms_dir = os.path.join(base_dir, sub_dir, class2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create directories\n",
    "The following script checks that if the folder already exists. If this is the first time to run this script it creates the directory structure for the train, test, and evaluation sets\n",
    "\n",
    "See also: \n",
    "- [Helper Python scripts](https://github.com/geekcomputers/Python)\n",
    "- [How to delete the contents of a folder in Python?](https://stackoverflow.com/questions/185936/how-to-delete-the-contents-of-a-folder-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\dataset2 already exists!\n"
     ]
    }
   ],
   "source": [
    "if not(os.path.exists(base_dir)):\n",
    "    print('Creating dataset folders to:', base_dir)\n",
    "    os.mkdir(base_dir)\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(train_nosymptoms_dir)\n",
    "    os.mkdir(train_symptoms_dir)\n",
    "    os.mkdir(validation_dir)\n",
    "    os.mkdir(validation_nosymptoms_dir)\n",
    "    os.mkdir(validation_symptoms_dir)\n",
    "    os.mkdir(test_dir)\n",
    "    os.mkdir(test_nosymptoms_dir)\n",
    "    os.mkdir(test_symptoms_dir)\n",
    "else:\n",
    "    print(base_dir, 'already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split the data filenames into train, validation and test sets\n",
    "Now when we have the directories ready, it's time to split the original dataset into training, validation and test sets. For that we use `scikit-learn` library's `train_test_split` function. First we split the dataset with rule 80%-20% and then we split 80% to 60% and 20%. Finally we get:\n",
    "- 60% training set\n",
    "- 20% validation set, and \n",
    "- 20% test set.\n",
    "\n",
    "This needs to be repeated both for the healthy (nosymptoms) and disease (symptom) cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 119, 119)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disease (symptom) cases split\n",
    "\n",
    "# Take 20 % out for testing\n",
    "train_symptoms_fnames, test_symptoms_fnames = train_test_split(symptoms_fnames, test_size = 0.2)\n",
    "\n",
    "# From the remaining 80% take 0.25 (=0.8*0.25 = 20% of total) out for validation\n",
    "train_symptoms_fnames, validation_symptoms_fnames = train_test_split(train_symptoms_fnames, test_size = 0.25)\n",
    "\n",
    "len(train_symptoms_fnames), len(validation_symptoms_fnames), len(test_symptoms_fnames)\n",
    "# For debugging purposes, remove the comment marks.\n",
    "# print(train_symptoms_fnames)\n",
    "# print(validation_symptoms_fnames)\n",
    "# print(test_symptoms_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(880, 294, 294)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Healthy (nosyptom) cases split\n",
    "\n",
    "# Take 20 % out for testing\n",
    "train_nosymptoms_fnames, test_nosymptoms_fnames = train_test_split(nosymptoms_fnames, test_size = 0.2)\n",
    "\n",
    "# From the remaining 80% take 0.25 (20% of total) out for validation\n",
    "train_nosymptoms_fnames, validation_nosymptoms_fnames = train_test_split(train_nosymptoms_fnames, test_size = 0.25)\n",
    "\n",
    "len(train_nosymptoms_fnames), len(validation_nosymptoms_fnames), len(test_nosymptoms_fnames)\n",
    "# For debugging purposes, remove the comment marks.\n",
    "# print(train_nosymptoms_fnames)\n",
    "# print(validation_nosymptoms_fnames)\n",
    "# print(test_nosymptoms_fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Copy data into the directories\n",
    "Last thing to do is to copy the original data into the training, validation and test directories. As this might take some time, we want to watch the time spend on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 72.42 sec\n"
     ]
    }
   ],
   "source": [
    "tStart = time.time()\n",
    "\n",
    "# Copy the original files into the dataset folders\n",
    "\n",
    "# Training set\n",
    "# Disease \n",
    "for fname in train_symptoms_fnames:\n",
    "    src = os.path.join(original_symptoms_dir, fname)\n",
    "    dst = os.path.join(train_symptoms_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "# Healthy \n",
    "for fname in train_nosymptoms_fnames:\n",
    "    src = os.path.join(original_nosymptoms_dir, fname)\n",
    "    dst = os.path.join(train_nosymptoms_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Validation set\n",
    "# Disease \n",
    "for fname in validation_symptoms_fnames:\n",
    "    src = os.path.join(original_symptoms_dir, fname)\n",
    "    dst = os.path.join(validation_symptoms_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "# Healthy\n",
    "for fname in validation_nosymptoms_fnames:\n",
    "    src = os.path.join(original_nosymptoms_dir, fname)\n",
    "    dst = os.path.join(validation_nosymptoms_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Test set\n",
    "# Disease\n",
    "for fname in test_symptoms_fnames:\n",
    "    src = os.path.join(original_symptoms_dir, fname)\n",
    "    dst = os.path.join(test_symptoms_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "# Healthy\n",
    "for fname in test_nosymptoms_fnames:\n",
    "    src = os.path.join(original_nosymptoms_dir, fname)\n",
    "    dst = os.path.join(test_nosymptoms_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "tStop = time.time()\n",
    "tElapsed = tStop - tStart\n",
    "print('Time elapsed: {:.2f} sec'.format(tElapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Now our case 2 directories and dataset are ready for training, validating and testing our convolutional neural networks to make predictions for retinopathy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
